"""LM Studio provider using native Python SDK."""

import base64
import time
from typing import Optional, List, Callable, Any

import lmstudio as lms

from .base import LLMProvider


class LMStudioNativeProvider(LLMProvider):
    """Provider for LM Studio using official Python SDK.
    
    Provides access to native LM Studio features including:
    - Native chat completions
    - Vision model support via prepare_image()
    - Better model metadata access
    - Streaming support (token-by-token via respond_stream())
    - Structured responses (future)
    - Agentic workflows (future)
    """
    
    # Native SDK's respond_stream() method provides real token-by-token streaming
    supports_streaming = True
    # Native SDK supports structured responses via response_format
    supports_structured_output = True
    
    def __init__(self, base_url: str = "localhost:1234"):
        """Initialize LM Studio native provider.
        
        Args:
            base_url: Host:port of LM Studio API server (default: localhost:1234)
        """
        self.base_url = base_url
        self._model_cache: List[str] = []
        self._model_cache_ts: float = 0.0
        self._loaded_cache: List[str] = []
        self._loaded_cache_ts: float = 0.0
        # Configure default client if needed
        if base_url != "localhost:1234":
            lms.configure_default_client(base_url)
        
        # Test connection
        self._test_connection()
    
    def _test_connection(self):
        """Verify LM Studio API server is running."""
        try:
            if not lms.Client.is_valid_api_host(self.base_url):
                print(f"Warning: No LM Studio API server found at {self.base_url}")
        except Exception as e:
            print(f"Warning: Could not verify LM Studio connection: {e}")
    
    def chat(self, messages: list, model: str = None, progress_callback: Optional[Callable[[Any], None]] = None, response_format: Optional[dict] = None) -> str:
        """Send chat message to LM Studio using native SDK.
        
        Args:
            messages: List of message dicts with 'role' and 'content'
                     Can include 'images' key for vision models
            model: Model name to use
            
        Returns:
            Response text from model
        """
        try:
            self._emit_progress(progress_callback, "connecting", self.base_url)

            # Get model handle
            if model:
                llm_model = lms.llm(model)
            else:
                llm_model = lms.llm()  # Use currently loaded model

            loaded_state = self.is_model_loaded(model) if model else None
            if loaded_state is False:
                self._emit_progress(progress_callback, "loading_model", model)
            elif loaded_state is True:
                self._emit_progress(progress_callback, "model_ready", model)

            # Build chat context
            chat = self._build_chat_context(messages)
            self._emit_progress(progress_callback, "sending")
            
            # Generate response
            # Generate response (structured if response_format is provided)
            if response_format is not None:
                result = llm_model.respond(chat, response_format=response_format)
            else:
                result = llm_model.respond(chat)
            self._emit_progress(progress_callback, "complete")
            # Return content as string; if structured returns dict-like, serialize
            content = getattr(result, 'content', result)
            try:
                import json
                if isinstance(content, (dict, list)):
                    return json.dumps(content, ensure_ascii=False, indent=2)
            except Exception:
                pass
            return str(content)
            
        except Exception as e:
            self._emit_progress(progress_callback, "error", str(e))
            return f"Error: {e}"
    
    def chat_stream(self, messages: list, model: str = None, progress_callback: Optional[Callable[[Any], None]] = None):
        """Stream chat response token-by-token using native SDK.
        
        Uses the native SDK's respond_stream() method to yield fragments
        as they are generated by the model. This provides true token-by-token
        streaming for responsive UX.
        
        Args:
            messages: List of message dicts with 'role' and 'content'
            model: Model name to use
            
        Yields:
            Response text tokens as they are generated
        """
        try:
            self._emit_progress(progress_callback, "connecting", self.base_url)
            # Get model handle
            if model:
                llm_model = lms.llm(model)
            else:
                llm_model = lms.llm()  # Use currently loaded model

            loaded_state = self.is_model_loaded(model) if model else None
            if loaded_state is False:
                self._emit_progress(progress_callback, "loading_model", model)
            elif loaded_state is True:
                self._emit_progress(progress_callback, "model_ready", model)
            
            # Build chat context
            chat = self._build_chat_context(messages)
            self._emit_progress(progress_callback, "receiving")
            
            # Stream response using native SDK
            for fragment in llm_model.respond_stream(chat):
                content = fragment.content if hasattr(fragment, 'content') else str(fragment)
                if content:
                    yield content
            self._emit_progress(progress_callback, "complete")
        except Exception as e:
            self._emit_progress(progress_callback, "error", str(e))
            yield f"Error: {e}"
    
    def _build_chat_context(self, messages: list):
        """Convert Inkwell message format to LM Studio Chat object.
        
        Args:
            messages: List of message dicts
            
        Returns:
            lms.Chat object
        """
        chat = lms.Chat()
        system_prompt = None
        
        for msg in messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            
            # Handle system messages
            if role == 'system':
                # Use first system message as chat's system prompt
                if system_prompt is None:
                    system_prompt = content
                    chat = lms.Chat(content)
                else:
                    # Add subsequent system messages as user messages
                    chat.add_user_message(f"System: {content}")
            
            # Handle user messages
            elif role == 'user':
                # Check for images (vision support)
                images = msg.get('images', [])
                if images:
                    # Convert base64 images to image handles
                    image_handles = []
                    for img_b64 in images:
                        try:
                            # Decode base64 to bytes
                            img_bytes = base64.b64decode(img_b64)
                            image_handle = lms.prepare_image(img_bytes)
                            image_handles.append(image_handle)
                        except Exception as e:
                            print(f"Warning: Could not prepare image: {e}")
                    
                    if image_handles:
                        chat.add_user_message(content, images=image_handles)
                    else:
                        chat.add_user_message(content)
                else:
                    chat.add_user_message(content)
            
            # Handle assistant messages (for conversation history)
            elif role == 'assistant':
                # Skip assistant messages in history for now
                # The SDK will generate fresh responses
                # TODO: Investigate if we can/should append assistant messages
                pass
        
        return chat
    
    def list_models(self, refresh: bool = False) -> List[str]:
        """List available models from LM Studio.
        
        Returns:
            List of model names
        """
        if not refresh and self._model_cache and (time.time() - self._model_cache_ts) < 15:
            return list(self._model_cache)
        try:
            import requests
            base = self._normalize_url(self.base_url)
            
            response = requests.get(f"{base}/v1/models", timeout=5)
            response.raise_for_status()
            data = response.json()
            
            models = []
            for m in data.get("data", []):
                mid = self._get_model_id(m)
                if mid:
                    models.append(mid)
            self._model_cache = models
            self._model_cache_ts = time.time()
            return models
            
        except Exception as e:
            print(f"Error listing LM Studio models: {e}")
            return []

    def get_loaded_models(self, refresh: bool = False) -> Optional[List[str]]:
        """Return models currently loaded by LM Studio when available.
        
        Returns None when the status cannot be determined.
        """
        if not refresh and self._loaded_cache and (time.time() - self._loaded_cache_ts) < 10:
            return list(self._loaded_cache)
        try:
            import requests
            base = self._normalize_url(self.base_url)
            response = requests.get(f"{base}/v1/models", timeout=5)
            response.raise_for_status()
            data = response.json()

            loaded: List[str] = []
            for m in data.get("data", []):
                mid = self._get_model_id(m)
                if not mid:
                    continue
                meta = m if isinstance(m, dict) else {}
                flags = [
                    meta.get("loaded"),
                    meta.get("isLoaded"),
                    meta.get("is_loaded"),
                    meta.get("isDefault"),
                    meta.get("default"),
                ]
                state = str(meta.get("state") or meta.get("status") or "").lower()
                if any(bool(f) for f in flags) or state in ("loaded", "ready", "active", "running"):
                    loaded.append(mid)
            if not loaded:
                ids = [self._get_model_id(m) for m in data.get("data", []) if self._get_model_id(m)]
                if len(ids) == 1:
                    loaded = ids
            self._loaded_cache = loaded
            self._loaded_cache_ts = time.time()
            return loaded
        except Exception as e:
            print(f"Error checking loaded LM Studio models: {e}")
            return None

    def is_model_loaded(self, model_name: Optional[str]) -> Optional[bool]:
        """Check if a specific model appears loaded.
        
        Returns:
            True if loaded, False if present but not loaded, None if unknown.
        """
        if not model_name:
            return None
        loaded_models = self.get_loaded_models()
        if loaded_models is None:
            return None
        return model_name in loaded_models
    
    def get_model_context_length(self, model_name: str) -> Optional[int]:
        """Get context length for a model.
        
        Args:
            model_name: Name of the model
            
        Returns:
            Context length in tokens, or None if unknown
        """
        # 1) Try native SDK method (most accurate for currently loaded model)
        try:
            llm_model = lms.llm(model_name) if model_name else lms.llm()
            ctx_len = llm_model.get_context_length()
            if ctx_len:
                return int(ctx_len)
        except Exception as e:
            print(f"DEBUG: Native get_context_length failed, falling back to REST: {e}")
        
        # 2) Fallback to REST metadata
        try:
            import requests
            base = self._normalize_url(self.base_url)

            response = requests.get(f"{base}/v1/models", timeout=5)
            response.raise_for_status()
            data = response.json()

            for m in data.get("data", []):
                if m.get("id") == model_name or m.get("model") == model_name:
                    return (m.get("max_model_len") or
                           m.get("context_length") or
                           m.get("max_context_length") or
                           m.get("n_ctx"))
            return None

        except Exception as e:
            print(f"Error getting model context length: {e}")
            return None
    
    def is_vision_model(self, model_name: str) -> bool:
        """Detect vision capability for model.
        
        Args:
            model_name: Name of the model
            
        Returns:
            True if model supports vision
        """
        if not model_name:
            return False
        
        try:
            # Use REST API to check metadata
            import requests
            base = self._normalize_url(self.base_url)
            
            response = requests.get(f"{base}/v1/models", timeout=5)
            response.raise_for_status()
            data = response.json()
            
            for m in data.get("data", []):
                if m.get("id") == model_name or m.get("model") == model_name:
                    # Check for vision capability in metadata
                    if m.get("vision"):
                        return True
                    # Check capabilities array
                    caps = m.get("capabilities", [])
                    if "vision" in caps or "image" in caps:
                        return True
            
            # Fall back to heuristic
            return super().is_vision_model(model_name)
            
        except Exception:
            # Fall back to base class heuristic
            return super().is_vision_model(model_name)
    
    def _normalize_url(self, base_url: str) -> str:
        """Normalize base URL to include http:// prefix.
        
        Args:
            base_url: Base URL (may be host:port or http://host:port)
            
        Returns:
            Normalized URL with http:// prefix
        """
        if base_url.startswith("http://") or base_url.startswith("https://"):
            return base_url.rstrip("/")
        else:
            return f"http://{base_url}"

    def _emit_progress(self, progress_callback: Optional[Callable[[Any], None]], phase: str, detail: Optional[str] = None):
        if not progress_callback:
            return
        payload = {"phase": phase}
        if detail:
            payload["detail"] = detail
        try:
            progress_callback(payload)
        except Exception:
            try:
                progress_callback(f"{phase}: {detail}" if detail else phase)
            except Exception:
                pass

    @staticmethod
    def _get_model_id(entry: dict) -> Optional[str]:
        if not isinstance(entry, dict):
            return None
        return entry.get("id") or entry.get("model") or entry.get("name")
